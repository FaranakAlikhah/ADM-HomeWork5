{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in ./opt/anaconda3/lib/python3.8/site-packages (2.4)\n",
      "Requirement already satisfied: decorator>=4.3.0 in ./opt/anaconda3/lib/python3.8/site-packages (from networkx) (4.4.2)\n",
      "Requirement already satisfied: fraction in ./opt/anaconda3/lib/python3.8/site-packages (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n",
    "!pip install fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import random\n",
    "from fraction import Fraction\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and Setting \n",
    "#### There are many options to collect and build the Wikipedia's underlying network, we rely on the dataset provided here. For the purpose of our exploration, we do not consider the entire dataset. Instead, we focus on the articles belonging to a subset of categories.\n",
    "\n",
    "Download the reduced version of the graph Wikicat hyperlink graph. Every row indicates an edge. In particular, the two elements are the source and the target, respectively.\n",
    "From this page download:\n",
    "wiki-topcats-categories.txt.gz: list of pages per category\n",
    "wiki-topcats-page-names.txt.gz: page names\n",
    "Note that in the reduced version of the network we removed the categories whose number of articles in less than 5000 and more than 30000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\"wikigraph_reduced.csv\", sep = \"\\t\")\n",
    "edges.columns = ['index', \"source\", \"dest\"] # update the column name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>1185516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>108</td>\n",
       "      <td>1059989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>108</td>\n",
       "      <td>1062426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>108</td>\n",
       "      <td>1161925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134</td>\n",
       "      <td>541222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   source     dest\n",
       "0      95  1185516\n",
       "1     108  1059989\n",
       "2     108  1062426\n",
       "3     108  1161925\n",
       "4     134   541222"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges.drop(['index'], axis = 1, inplace = True)\n",
    "edges.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Pages List</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buprestoidea</td>\n",
       "      <td>[301, 302, 303, 304, 305, 306, 307, 308, 309, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>People_from_Worcester</td>\n",
       "      <td>[1056, 1057, 1058, 1059, 1060, 60971, 76515, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skin_conditions_resulting_from_physical_factors</td>\n",
       "      <td>[971, 973, 1166, 1167, 1168, 1169, 1170, 1171,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Visual_kei_bands</td>\n",
       "      <td>[1297, 1300, 1311, 1312, 1313, 1314, 1315, 131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Japanese_rock_music_groups</td>\n",
       "      <td>[1297, 1300, 1313, 1314, 1315, 1316, 1319, 132...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17359</th>\n",
       "      <td>British_science_fiction_novels</td>\n",
       "      <td>[16110, 24545, 39814, 67923, 108629, 108630, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17360</th>\n",
       "      <td>Television_soundtracks</td>\n",
       "      <td>[1117, 110192, 110376, 110377, 110380, 110395,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17361</th>\n",
       "      <td>Diptera_of_North_America</td>\n",
       "      <td>[279, 280, 283, 31110, 31129, 31133, 31138, 31...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17362</th>\n",
       "      <td>Diptera_of_Asia</td>\n",
       "      <td>[279, 31129, 31133, 31134, 31135, 31136, 31137...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17363</th>\n",
       "      <td>Tachinidae</td>\n",
       "      <td>[429765, 429776, 860398, 1427707, 1427710, 142...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17364 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Category  \\\n",
       "0                                         Buprestoidea   \n",
       "1                                People_from_Worcester   \n",
       "2      Skin_conditions_resulting_from_physical_factors   \n",
       "3                                     Visual_kei_bands   \n",
       "4                           Japanese_rock_music_groups   \n",
       "...                                                ...   \n",
       "17359                   British_science_fiction_novels   \n",
       "17360                           Television_soundtracks   \n",
       "17361                         Diptera_of_North_America   \n",
       "17362                                  Diptera_of_Asia   \n",
       "17363                                       Tachinidae   \n",
       "\n",
       "                                              Pages List  \n",
       "0      [301, 302, 303, 304, 305, 306, 307, 308, 309, ...  \n",
       "1      [1056, 1057, 1058, 1059, 1060, 60971, 76515, 7...  \n",
       "2      [971, 973, 1166, 1167, 1168, 1169, 1170, 1171,...  \n",
       "3      [1297, 1300, 1311, 1312, 1313, 1314, 1315, 131...  \n",
       "4      [1297, 1300, 1313, 1314, 1315, 1316, 1319, 132...  \n",
       "...                                                  ...  \n",
       "17359  [16110, 24545, 39814, 67923, 108629, 108630, 1...  \n",
       "17360  [1117, 110192, 110376, 110377, 110380, 110395,...  \n",
       "17361  [279, 280, 283, 31110, 31129, 31133, 31138, 31...  \n",
       "17362  [279, 31129, 31133, 31134, 31135, 31136, 31137...  \n",
       "17363  [429765, 429776, 860398, 1427707, 1427710, 142...  \n",
       "\n",
       "[17364 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = pd.read_csv(\"wiki-topcats-categories.txt\", sep=\";\", names = [\"Category\", \"Pages List\"])\n",
    "# cleaning data deviding to two columns \n",
    "categories[\"Category\"] = categories.Category.apply(lambda x: x[9:]) # creating category columns \n",
    "categories[\"Pages List\"] = categories[\"Pages List\"].apply(lambda x: x.split()) # creating page_list columns\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download page_names  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1791489it [00:01, 1004454.18it/s]\n"
     ]
    }
   ],
   "source": [
    "col_1 = []\n",
    "col_2 = []\n",
    "f=  open (r'wiki-topcats-page-names.txt', 'r')\n",
    "count = 0\n",
    "for file in tqdm(f):\n",
    "    z = file.split(' ',maxsplit = 1)\n",
    "    col_1.append(z[0])\n",
    "    col_2.append(z[1].replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_names = pd.DataFrame()\n",
    "page_names['index'] = col_1\n",
    "page_names['name'] =  col_2\n",
    "page_names.to_csv('page_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_names = pd.read_csv('page_names.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the graph G=(V, E), where V is the set of articles and E the hyperlinks among them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483094/483094 [02:08<00:00, 3748.50it/s]\n"
     ]
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "for i in tqdm(range(len(edges[:]))):\n",
    "    G.add_edge(edges.loc[i][0], edges.loc[i][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the graph directed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see from the edges dataframe graph is directed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the edges dataframe graph is directed.\n",
    "nx.draw(G, node_size=0.2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many articles are we considering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of article are 98343\n"
     ]
    }
   ],
   "source": [
    "# Number of articles\n",
    "print('No of article are {}'.format(len(set.union(set(edges['source'].unique()), set(edges['dest'].unique())))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many hyperlinks between pages exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of edges are 483094\n"
     ]
    }
   ],
   "source": [
    "# Number of edges\n",
    "print('No of edges are {}'.format( len(edges)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the average number of links in an arbitrary page. What is the graph density?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.824674862471147"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the average number of links in an arbitrary page\n",
    "np.mean(list(dict(G.degree).values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you believe that the graph is dense or sparse. What is the graph density?\n",
    "\n",
    "\n",
    "#### As we can see the value of density is small hence we can conlude that it is sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Density of graph is 4.9951571365597335e-05\n"
     ]
    }
   ],
   "source": [
    "# Let's calculate density of a graph,\n",
    "# As we can see the value of density is small hence we can conlude that it is sparse\n",
    "n = len(G.nodes)\n",
    "m = len(G.edges)\n",
    "print(\"Density of graph is {}\".format(m/(n*(n-1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you believe that the graph is dense or sparse? Is the graph dense? Visualize the nodes' degree distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLotting degree didtrubution \n",
    "def degree_dist(G):\n",
    "    d = [G.degree(n) for n in G.nodes()]\n",
    "    plt.hist(d)\n",
    "    plt.show()\n",
    "\n",
    "degree_dist(nx.gnp_random_graph(100, 0.5\n",
    "                                , directed=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.2 :\n",
    "\n",
    "### Define a function that takes in input :\n",
    "\n",
    "A page v\n",
    "A number of clicks d\n",
    "and returns the set of all pages that a user can reach within d clicks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### at first step we define more convinient version of edge_dict dictionary witch has key values equal to sources and corresponding value equal to neighbours and first destination of that source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node = list(edges[\"source\"])\n",
    "dest_node = list(edges[\"dest\"])\n",
    "links = []\n",
    "for i,j in zip(edges['source'],edges['dest']):\n",
    "    links.append((i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_dict = defaultdict(list)\n",
    "for i in range(len(source_node)):\n",
    "    edge_dict[source_node[i]].append(dest_node[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def No_of_clicks(given_list, c):\n",
    "    b = c\n",
    "    given_list = given_list\n",
    "    click = []\n",
    "    list_std = []\n",
    "    while b >0 :\n",
    "        z = []\n",
    "        for i in tqdm(given_list):\n",
    "        #         print(i)\n",
    "            try:        \n",
    "                for j in edge_dict[i]: \n",
    "                    z.append(j)\n",
    "                    click.append(j)       \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        given_list =z\n",
    "        b = b - 1 \n",
    "    return click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node = list(edges[\"source\"])\n",
    "dest_node = list(edges[\"dest\"])\n",
    "union_node = list(set.union(set(source_node),set(dest_node)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please enter the source node 134\n",
      "How many clicks you want 2\n"
     ]
    }
   ],
   "source": [
    "# asking proper inputs from user \n",
    "source_node = int(input('please enter the source node '))\n",
    "c = int(input('How many clicks you want '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 799.37it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 12738.96it/s]\n"
     ]
    }
   ],
   "source": [
    "given_list = [source_node]\n",
    "Total_nodes_reched = No_of_clicks(given_list, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[541222, 1061485, 1163610, 1163783, 536864, 538134, 543070, 1110882, 134, 62637, 355747, 358205, 530893, 823545, 1025352, 1025885, 1026878, 1053045, 1056077, 1056294, 1056873, 1056874, 1056964, 1056966, 1057565, 1057624, 1057685, 1058035, 1058047, 1058420, 1058860, 1059622, 1060082, 1060600, 1060768, 1061245, 1061249, 1061352, 1061422, 1061427, 1061444, 1061445, 1061450, 1061452, 1061461, 1061463, 1061466, 1061486, 1061487, 1061488, 1061489, 1061494, 1061502, 1061503, 1061506, 1061507, 1061552, 1061564, 1061565, 1061606, 1061902, 1062637, 1062705, 1063007, 1065425, 1065712, 1065725, 1066078, 1162401, 1163140, 1163675, 1163706, 1163711, 1163712, 1163713, 1163717, 1163722, 1163723, 1163727, 1163732, 1163733, 1163735, 1163744, 1163754, 1246598, 1265236, 1265356, 1265367, 1265376, 1265422, 1266105, 1266113, 1266608, 1270720, 1400636, 1502817, 134, 19415, 339891, 573333, 574081, 636021, 696881, 771113, 789467, 925690, 930923, 1028354, 1049218, 1049509, 1050418, 1052967, 1056873, 1057478, 1061354, 1061486, 1061506, 1061730, 1062224, 1064355, 1068441, 1069712, 1070760, 1083937, 1151062, 1162493, 1162514, 1163727, 1163776, 1213613, 1246623, 1247965, 1265363, 1265498, 1265512, 1265513, 1265514, 1265515, 1265530, 1265532, 1265605, 1266019, 1266040, 1266209, 1266267, 1266367, 1266395, 1266429, 1266473, 1266708, 1469661, 1557615, 1557720, 134, 330470, 338556, 1024679, 1049479, 1049493, 1052967, 1053000, 1053949, 1053965, 1054038, 1056873, 1058032, 1061485, 1061486, 1061487, 1061488, 1061489, 1064396, 1068028, 1069690, 1070042, 1081615, 1162736, 1163695, 1163722, 1163727, 1164631, 1166598, 1223481, 1223584, 1246417, 1246598, 1246825, 1247025, 1264487, 1264489, 1264875, 1266804, 1270455, 1400636, 1469661, 1557713, 1628696, 1786953]\n"
     ]
    }
   ],
   "source": [
    "# No of nodes can be reached with given clicks\n",
    "print(Total_nodes_reched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.\n",
    "#### Define a function that takes in input:\n",
    "\n",
    "A category C\n",
    "A set of pages in C, p = {p1, ..., pn}\n",
    "and returns the minimum number of clicks required to reach all pages in p, starting from the page v, corresponding to the most central article, according to the in-degree centrality, in C.\n",
    "\n",
    "Consider that:\n",
    "\n",
    "The algorithm needs to handle the case that the graph is not connected, thus not all the pages in p are reachable from v. In such scenario, it is enough to let the program give in output the string \"Not possible\".\n",
    "Since we are dealing with graph exploration, you can pass more than once on the same page pi.\n",
    "Since the problemâ€™s complexity is high, consider to provide just an approximation/heuristic solution for the problem.\n",
    "You can use whatever metrics of centrality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_counter(source , dest , edge_dict, keys):\n",
    "\n",
    "    queue = []\n",
    "    counter = 0\n",
    "    visited = defaultdict()\n",
    "    for key in keys : \n",
    "        visited[key] = False\n",
    "        for i in edge_dict[key]:\n",
    "            visited[i] = False\n",
    "\n",
    "    \n",
    "    queue.append(source)\n",
    "    visited [source] = True\n",
    "    if source != dest:\n",
    "        \n",
    "        while (len(queue) != 0) :\n",
    "\n",
    "            u = queue[0] # start from first elemnt available at queue list \n",
    "            queue.pop(0)# delete this from queue and return it \n",
    "            neighbour = edge_dict[u]\n",
    "\n",
    "            for i in neighbour:\n",
    "                if visited[i] == False :\n",
    "\n",
    "                    visited[i] = True \n",
    "                    queue.append(i) \n",
    "                    counter += 1 #to calculate path length \n",
    "\n",
    "                    if (i == dest):\n",
    "                        return counter\n",
    "    \n",
    "    counter = np.infty\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_clicks(cat, edge_dict):\n",
    "    dictionary_of_indegree = dict(edges['source'].value_counts())\n",
    "    maxi = []\n",
    "\n",
    "    the_list =  [int(i) for i in cat['Pages List'][0]]\n",
    "    for i in the_list:\n",
    "        try:\n",
    "            maxi.append(( dictionary_of_indegree[i], i))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    if len(maxi)>0 :\n",
    "        start = max(maxi)[1]\n",
    "\n",
    "        keys = edge_dict.keys()\n",
    "        for i in the_list:\n",
    "            counter = bfs_counter(start , i , edge_dict, keys )\n",
    "            if counter != np.inf:\n",
    "                print('Clicks required from {} to {} is {}'.format(start, i, counter))\n",
    "            else:\n",
    "                print('Not possible from {} to {}'.format(start, i))\n",
    "    else:\n",
    "        print(' Exception:  You don\"t have node with in degree ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter thr category People_from_Worcester\n",
      "Not possible from 540020 to 1056\n",
      "Not possible from 540020 to 1057\n",
      "Not possible from 540020 to 1058\n",
      "Not possible from 540020 to 1059\n",
      "Not possible from 540020 to 1060\n",
      "Not possible from 540020 to 60971\n",
      "Not possible from 540020 to 76515\n",
      "Not possible from 540020 to 76871\n",
      "Not possible from 540020 to 78094\n",
      "Clicks required from 540020 to 79069 is 44247\n",
      "Not possible from 540020 to 79139\n",
      "Clicks required from 540020 to 79143 is 51852\n",
      "Not possible from 540020 to 80237\n",
      "Not possible from 540020 to 84108\n",
      "Not possible from 540020 to 84354\n",
      "Not possible from 540020 to 84740\n",
      "Not possible from 540020 to 85268\n",
      "Not possible from 540020 to 85767\n",
      "Clicks required from 540020 to 89734 is 51843\n",
      "Not possible from 540020 to 90171\n",
      "Not possible from 540020 to 95330\n",
      "Not possible from 540020 to 158412\n",
      "Not possible from 540020 to 166087\n",
      "Not possible from 540020 to 194583\n",
      "Not possible from 540020 to 196217\n",
      "Not possible from 540020 to 216650\n",
      "Not possible from 540020 to 216770\n",
      "Not possible from 540020 to 348758\n",
      "Not possible from 540020 to 350247\n",
      "Not possible from 540020 to 350259\n",
      "Not possible from 540020 to 350608\n",
      "Not possible from 540020 to 372883\n",
      "Not possible from 540020 to 377480\n",
      "Not possible from 540020 to 437692\n",
      "Not possible from 540020 to 449761\n",
      "Not possible from 540020 to 449809\n",
      "Not possible from 540020 to 454664\n",
      "Not possible from 540020 to 494420\n",
      "Not possible from 540020 to 499532\n",
      "Not possible from 540020 to 527094\n",
      "Clicks required from 540020 to 537220 is 48455\n",
      "Clicks required from 540020 to 538870 is 1\n",
      "Not possible from 540020 to 540020\n",
      "Clicks required from 540020 to 541169 is 37817\n",
      "Not possible from 540020 to 544700\n",
      "Not possible from 540020 to 544701\n",
      "Not possible from 540020 to 544738\n",
      "Not possible from 540020 to 544888\n",
      "Not possible from 540020 to 546749\n",
      "Not possible from 540020 to 654238\n",
      "Not possible from 540020 to 669757\n",
      "Not possible from 540020 to 721089\n",
      "Not possible from 540020 to 721539\n",
      "Not possible from 540020 to 791902\n",
      "Not possible from 540020 to 799357\n",
      "Not possible from 540020 to 826926\n",
      "Not possible from 540020 to 827472\n",
      "Not possible from 540020 to 827479\n",
      "Not possible from 540020 to 830268\n",
      "Not possible from 540020 to 883273\n",
      "Not possible from 540020 to 919797\n",
      "Not possible from 540020 to 971629\n",
      "Not possible from 540020 to 979026\n",
      "Not possible from 540020 to 995033\n",
      "Not possible from 540020 to 998609\n",
      "Not possible from 540020 to 1011834\n",
      "Not possible from 540020 to 1033464\n",
      "Not possible from 540020 to 1043638\n",
      "Not possible from 540020 to 1043983\n",
      "Not possible from 540020 to 1047370\n",
      "Not possible from 540020 to 1112567\n",
      "Not possible from 540020 to 1142375\n",
      "Not possible from 540020 to 1143260\n",
      "Not possible from 540020 to 1170545\n",
      "Not possible from 540020 to 1178634\n",
      "Not possible from 540020 to 1194242\n",
      "Not possible from 540020 to 1260349\n",
      "Not possible from 540020 to 1284729\n",
      "Not possible from 540020 to 1287891\n",
      "Not possible from 540020 to 1287952\n",
      "Not possible from 540020 to 1342966\n",
      "Not possible from 540020 to 1343202\n",
      "Not possible from 540020 to 1385068\n",
      "Not possible from 540020 to 1390440\n",
      "Not possible from 540020 to 1445619\n",
      "Not possible from 540020 to 1470844\n",
      "Not possible from 540020 to 1472916\n",
      "Not possible from 540020 to 1499171\n",
      "Not possible from 540020 to 1506216\n",
      "Not possible from 540020 to 1516445\n",
      "Not possible from 540020 to 1525160\n",
      "Not possible from 540020 to 1525827\n",
      "Not possible from 540020 to 1527616\n",
      "Not possible from 540020 to 1565695\n",
      "Not possible from 540020 to 1568124\n",
      "Not possible from 540020 to 1610622\n",
      "Not possible from 540020 to 1646864\n",
      "Not possible from 540020 to 1653449\n",
      "Not possible from 540020 to 1667832\n",
      "Not possible from 540020 to 1715203\n",
      "Not possible from 540020 to 1779646\n",
      "Not possible from 540020 to 1780461\n",
      "Not possible from 540020 to 1781093\n"
     ]
    }
   ],
   "source": [
    "cat = input('Please enter thr category ')\n",
    "if cat in list(categories['Category']):\n",
    "    cat = categories[categories['Category'] ==  cat].reset_index(drop=True)\n",
    "    cat_clicks(cat, edge_dict)\n",
    "else:\n",
    "    print(' Exception:  INvalid category please try again ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4.\n",
    "#### Given in input two categories: C1 and C2, we get the subgraph induced by all the articles in the two categories.\n",
    "\n",
    "Let v and u two arbitrary pages in the subgraph. What is the minimum set of hyperlinks one can remove to disconnect u and v?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Explanations about our approach :\n",
    "\n",
    "at first we define bfs function according to the algorithm we talked about during the course and then we define extra function path_finder through these two dunctions at the first we calculate the shortest path adn then we find the exact address od path and by deleting set of hyperlinks related to this path we check again is there any other possible second shorts path or not if it exist after deleting this existance path we check again until two concidered nodes get disconnected. so the output of our main function is the number of shortest path we need to delete to make corresponding nodes disconnected \n",
    "\n",
    "\n",
    "all of these calculationd are apply to sungraph of choosen two classes by user and just includes all available nodes in these two classes and two nodes are choosen randomly consequtively from two classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(source , dest , edge_dict , distance = defaultdict(),parent = defaultdict()):\n",
    "    queue = []\n",
    "    visited = defaultdict()\n",
    "    distance = defaultdict()\n",
    "    parent = defaultdict()\n",
    "   \n",
    "    \n",
    "    if source == dest:\n",
    "        return (False, print('Source and dest are same'))\n",
    "    \n",
    "    for node in edge_dict.keys():\n",
    "        visited[node] = False\n",
    "        distance[node] = np.inf\n",
    "        parent [node] = -1\n",
    "    for key in edge_dict.keys():\n",
    "        for i in range(len(edge_dict[key])): \n",
    "            node = edge_dict[key][i]\n",
    "            visited[node] = False\n",
    "            distance[node] = np.inf\n",
    "            parent [node] = -1\n",
    "            \n",
    "    queue.append(source)\n",
    "    visited [source] = True\n",
    "    while (len(queue) != 0) :\n",
    "        u = queue[0] # start from first elemnt available at queue list \n",
    "        queue.pop(0)# delete this from queue and return it \n",
    "        neighbour = edge_dict[u]\n",
    "        if len(neighbour) != 0:\n",
    "            for i in range(len(neighbour)):\n",
    "                if visited[neighbour[i]] == False :\n",
    "                    visited[neighbour[i]] = True \n",
    "                    distance[neighbour[i]] = distance[u] + 1\n",
    "                    parent[neighbour[i]] = u\n",
    "                    queue.append(neighbour[i]) \n",
    "\n",
    "                    if (neighbour[i] == dest):\n",
    "                        return True , parent\n",
    "\n",
    "        \n",
    "    return False,parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_finder(source , dest , edge_dict):\n",
    "    \n",
    "    [result , parent] = bfs(source , dest , edge_dict , distance = defaultdict(),parent = defaultdict())\n",
    "    \n",
    "    if (result == False):\n",
    "        print(\"Not Connected\")\n",
    "  \n",
    "    # vector path stores the shortest path\n",
    "    if (result == True):\n",
    "        \n",
    "        path = defaultdict(list)\n",
    "        mid_node = dest\n",
    "        path[mid_node].append(\"1\")\n",
    "        \n",
    "\n",
    "    for key in parent.keys():\n",
    "        if (parent[key] != -1):\n",
    "            if parent[key] not in path.values():\n",
    "            \n",
    "                path[parent[key]].append(\"1\")\n",
    "\n",
    "\n",
    "\n",
    "    return path.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_path_delete_to_disconnect(first_node, second_node, di):\n",
    "    if first_node != second_node:\n",
    "        dict_ = di\n",
    "        counter = 0\n",
    "        while (bfs(first_node , second_node,dict_)[0]==True):\n",
    "            zz = path_finder(first_node , second_node, dict_)\n",
    "            counter +=1\n",
    "            new_data = edges[(~edges['source'].isin(list(zz)[1:]) )|(~edges['dest'].isin(list(zz)[1:]))]\n",
    "        #     new_data = edges[~edges['dest'].isin(list(zz)[1:])]\n",
    "            new_data.reset_index(drop=True, inplace = True)\n",
    "            dict_ = defaultdict(list)\n",
    "\n",
    "            for i in range(len(new_data)):\n",
    "                dict_[new_data['source'][i]].append(new_data['dest'][i])\n",
    "            edges = new_data\n",
    "        \n",
    "        if counter ==0 :\n",
    "            print('Randomly choosen node are not connected')\n",
    "        else:\n",
    "            print(\"The minimum number of required path to delete connection between two node \",counter)\n",
    "    else:\n",
    "        print('Source and dest are same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter first category Buprestoidea\n",
      "Please enter first category People_from_Worcester\n"
     ]
    }
   ],
   "source": [
    "cat_1 = input('Please enter first category ')\n",
    "cat_2 = input('Please enter first category ')\n",
    "if cat_1 == cat_2:\n",
    "    print('Please enter two different categories')\n",
    "elif cat_1 not in list(categories['Category']):\n",
    "    print('Please enter valid cat1')\n",
    "elif cat_2 not in list(categories['Category']):\n",
    "    print('Please enter valid cat2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly choosen node are not connected\n"
     ]
    }
   ],
   "source": [
    "cat_1 =categories[categories['Category']== cat_1].reset_index(drop = True)['Pages List'][0]\n",
    "cat_2 = categories[categories['Category']== cat_2].reset_index(drop = True)['Pages List'][0]\n",
    "cat_1 = [int(i) for i in cat_1]\n",
    "cat_2 = [int(i) for i in cat_2]\n",
    "\n",
    "\n",
    "# we are assuming that , They may be connected with the subgraph\n",
    "temp_df = edges[edges['source'].isin(cat_1+cat_2) | edges['dest'].isin(cat_1 + cat_2)].reset_index(drop=True)\n",
    "source_node_cat = list(temp_df[\"source\"])\n",
    "dest_node_cat = list(temp_df[\"dest\"])\n",
    "edge_dict_cat = defaultdict(list)\n",
    "for i in range(len(source_node_cat)):\n",
    "    edge_dict_cat[source_node_cat[i]].append(dest_node_cat[i])\n",
    "u = random.choice(cat_1)\n",
    "v = random.choice(cat_2)\n",
    "\n",
    "find_min_path_delete_to_disconnect(u, v, edge_dict_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5.\n",
    "#### Write a function that, given an arbitrary category C0 as input, returns the list of remaning categories sorted by their distance from C0. In particular, the distance between two categories is defined as\n",
    "\n",
    "distance(C0, Ci) = median(ShortestPath(C0, Ci))\n",
    "\n",
    "where ShortestPath(C0, Ci) is the set of shortest paths from each pair of nodes in the two categories.\n",
    "\n",
    "\n",
    "### Brief Explanations about our approach :\n",
    "\n",
    "In this question we worked with limited version of categories data frame wich just includes page list with values between 5000 and 30000 and also nodes in this range.\n",
    "\n",
    "our function get as an input the category choosen by user and category data frame and as an out put we will see data frame that sorts categories with respect to their distance with choosen category "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90920, 77)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_ = edges.copy(deep=True)\n",
    "edges_ = edges_[edges_['source']>5000]\n",
    "edges_ = edges_[edges_['source']<30000]\n",
    "edges_ = edges_[edges_['dest']>5000]\n",
    "edges_ = edges_[edges_['dest']<30000]\n",
    "edges_\n",
    "source_node_ = list(edges_[\"source\"])\n",
    "dest_node_ = list(edges_[\"dest\"])\n",
    "edge_dict_re = defaultdict(list)\n",
    "for i in range(len(source_node_)):\n",
    "    edge_dict_re[source_node_[i]].append(dest_node_[i])\n",
    "len(edge_dict),len(edge_dict_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_reducer(cat):\n",
    "    main_cat =categories[categories['Category']== cat].reset_index(drop = True)['Pages List'][0]\n",
    "# cat_1 = [int(i) for i in main_cat]\n",
    "    cat_ = [int(i) for i in main_cat if int(i)<30000 and int(i)>5000]\n",
    "    return cat_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = categories.copy(deep=True)\n",
    "category['Reduce_Page_list'] = category['Category'].apply(category_reducer)\n",
    "category = category[category['Reduce_Page_list'].map(lambda d: len(d) > 0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs_counter(source , dest , edge_dict, keys,visited ):\n",
    "    \n",
    "    queue = []\n",
    "    counter = 0\n",
    "    queue.append(source)\n",
    "\n",
    "    visited [source] = True\n",
    "    if source != dest:\n",
    "        \n",
    "        while (len(queue) != 0) :\n",
    "\n",
    "            u = queue[0] # start from first elemnt available at queue list \n",
    "            queue.pop(0)# delete this from queue and return it \n",
    "            neighbour = edge_dict[u]\n",
    "\n",
    "            for i in neighbour:\n",
    "                if visited[i] == False :\n",
    "\n",
    "                    visited[i] = True \n",
    "                    queue.append(i) \n",
    "                    counter += 1 #to calculate path length \n",
    "\n",
    "                    if (i == dest):\n",
    "                        return counter\n",
    "    \n",
    "    counter = np.infty\n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = categories.copy(deep=True)\n",
    "category['Reduce_Page_list'] = category['Category'].apply(category_reducer)\n",
    "category = category[category['Reduce_Page_list'].map(lambda d: len(d) > 0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_sorter(ask_cat , category ,edge_dict_re):   \n",
    "    main_cat =category[category['Category']== ask_cat].reset_index(drop = True)['Pages List'][0]\n",
    "    \n",
    "    cat_1 = [int(i) for i in main_cat if int(i)<30000 and int(i)>5000]\n",
    "\n",
    "    all_cat = category[~(category['Category']== ask_cat)]\n",
    "  \n",
    "    a=[]\n",
    "    b=[]\n",
    "    for cat in tqdm(all_cat['Category']) :\n",
    "\n",
    "        cat_2 = category[category['Category']== cat ].reset_index(drop = True)['Reduce_Page_list'][0]\n",
    "\n",
    "        temp_df = edges_[edges_['source'].isin(cat_1+cat_2) | edges_['dest'].isin(cat_1 + cat_2)].reset_index(drop=True)\n",
    "        source_node_cat = list(temp_df[\"source\"])\n",
    "        dest_node_cat = list(temp_df[\"dest\"])\n",
    "\n",
    "        edge_dict_cat = defaultdict(list)\n",
    "        for i in range(len(source_node_cat)):\n",
    "            edge_dict_cat[source_node_cat[i]].append(dest_node_cat[i])\n",
    "\n",
    "        median_collector = []\n",
    "\n",
    "        keys = edge_dict_re.keys()\n",
    "        visited = defaultdict()\n",
    "        for key in keys : \n",
    "            visited[key] = False\n",
    "            for i in edge_dict_re[key]:\n",
    "                visited[i] = False\n",
    "\n",
    "        for source in source_node_cat:\n",
    "            for dest in dest_node_cat:\n",
    "\n",
    "                visit =  copy.deepcopy(visited)\n",
    "                counter = bfs_counter(source , dest , edge_dict_re, keys,visit)\n",
    "\n",
    "                if counter != np.inf :\n",
    "                    median_collector.append(counter)\n",
    "\n",
    "                    a.append([counter,source,dest])\n",
    "\n",
    "        if len(median_collector)>=1 :\n",
    "            b.append(('British_science_fiction_novels' , cat , np.median(median_collector)))\n",
    "    cat_name= []\n",
    "    distance = []\n",
    "    for i in b :\n",
    "        cat_name.append(i[1])\n",
    "        distance.append(i[2])\n",
    "    sorted_df = pd.DataFrame()\n",
    "    sorted_df['cat_name'] = cat_name\n",
    "    sorted_df['distance'] = distance\n",
    "    sorted_df.sort_values(by=['distance'],ascending=False)\n",
    "    \n",
    "\n",
    "    return(sorted_df.sort_values(by=['distance'],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British_science_fiction_novels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4939/4939 [00:17<00:00, 279.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_name</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Slovakia_international_footballers</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Slovak_footballers</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>OFK_Beograd_players</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Portuguese_expatriate_footballers</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Estrela_da_Amadora_players</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>FC_Sion_players</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Computer_programmers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>American_technology_writers</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Year_of_death_missing</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>American_television_films</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               cat_name  distance\n",
       "260  Slovakia_international_footballers      11.0\n",
       "261                  Slovak_footballers      11.0\n",
       "141                 OFK_Beograd_players       9.0\n",
       "152   Portuguese_expatriate_footballers       7.0\n",
       "160          Estrela_da_Amadora_players       7.0\n",
       "..                                  ...       ...\n",
       "118                     FC_Sion_players       1.0\n",
       "119                Computer_programmers       1.0\n",
       "120         American_technology_writers       1.0\n",
       "121               Year_of_death_missing       1.0\n",
       "266           American_television_films       1.0\n",
       "\n",
       "[267 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ask_cat = input()\n",
    "if ask_cat not in list(categories['Category']):\n",
    "    print('Please enter valid cat1')\n",
    "else:\n",
    "    print(category_sorter(ask_cat , category ,edge_dict_re))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q_6 :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ranking_Page(category,edges,num_iter): \n",
    "    source_node = list(edges[\"source\"])\n",
    "    dest_node = list(edges[\"dest\"])\n",
    "\n",
    "    neighbour_dict = defaultdict(list)\n",
    "    for i in range(len(source_node)):\n",
    "        neighbour_dict[source_node[i]].append(dest_node[i])\n",
    "\n",
    "    neighbour_dict_dest = defaultdict(list)\n",
    "    for i in range(len(source_node)):\n",
    "        neighbour_dict_dest[dest_node[i]].append(source_node[i])\n",
    "\n",
    "    union_node = list(set.union(set(source_node),set(dest_node)))\n",
    "\n",
    "    dangeling=[]\n",
    "    for key in union_node:\n",
    "        if len(neighbour_dict_dest[key])==0 :\n",
    "            dangeling.append(key)\n",
    "\n",
    "    iter_dict = {}\n",
    "    for node in union_node:\n",
    "        iter_dict[node] =Fraction(1,len(union_node))\n",
    "    for i in tqdm(num_iter):\n",
    "        counter = 0\n",
    "        iter_dict2 = copy.deepcopy(iter_dict)\n",
    "\n",
    "        for key in union_node:\n",
    "\n",
    "            if len(neighbour_dict_dest[key])>0 :\n",
    "\n",
    "                s =Fraction(0)\n",
    "                for conect in neighbour_dict_dest[key]:\n",
    "\n",
    "                    s = s + iter_dict[conect]/Fraction(len(neighbour_dict[conect]))\n",
    "            else :\n",
    "                counter +=1\n",
    "                s = Fraction(0)\n",
    "\n",
    "\n",
    "            iter_dict2[key] = s\n",
    "        check = Fraction(0)\n",
    "        for  key in iter_dict2 :\n",
    "            check= check+iter_dict2[key]\n",
    "        for d in dangeling :\n",
    "            iter_dict2[d] = (Fraction(1)-check)/Fraction(len(dangeling))\n",
    "\n",
    "        iter_dict = copy.deepcopy(iter_dict2)\n",
    "\n",
    "\n",
    "    page_rank = defaultdict(list)\n",
    "    for cat in tqdm(category['Category']):\n",
    "        for node in list(category[category['Category']==cat]['Reduce_Page_list'])[0]:\n",
    "            if int(node) in union_node:\n",
    "                page_rank[cat].append(iter_dict2[int(node)])\n",
    "\n",
    "\n",
    "    category_name = {}\n",
    "    # rank = []\n",
    "    for key in page_rank:\n",
    "    #     category_name .append(key)\n",
    "        j=Fraction(0)\n",
    "        for k in range(len(page_rank[key])) :\n",
    "            j=j+page_rank[key][k]\n",
    "        category_name[key] = j\n",
    "\n",
    "    sorted_page_rank = pd.DataFrame()\n",
    "    sorted_page_rank['cat_name'] = category_name.keys()\n",
    "    sorted_page_rank['Page_rank'] = category_name.values()\n",
    "    return sorted_page_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:22<00:00,  5.60s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4940/4940 [02:30<00:00, 32.87it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_name</th>\n",
       "      <th>Page_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Molecular_biologists</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American_geneticists</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nobel_laureates_in_Physiology_or_Medicine</td>\n",
       "      <td>2/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Strategic_Air_Command</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Recipients_of_the_Air_Medal</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>University_of_Chicago_alumni</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Harvard_University_faculty</td>\n",
       "      <td>1/32781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>University_of_Chicago_faculty</td>\n",
       "      <td>2/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>American_television_films</td>\n",
       "      <td>4/98343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1951_films</td>\n",
       "      <td>1/98343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      cat_name Page_rank\n",
       "0                         Molecular_biologists   1/98343\n",
       "1                         American_geneticists   1/98343\n",
       "2    Nobel_laureates_in_Physiology_or_Medicine   2/98343\n",
       "3                        Strategic_Air_Command   1/98343\n",
       "4                  Recipients_of_the_Air_Medal   1/98343\n",
       "..                                         ...       ...\n",
       "533               University_of_Chicago_alumni   1/98343\n",
       "534                 Harvard_University_faculty   1/32781\n",
       "535              University_of_Chicago_faculty   2/98343\n",
       "536                  American_television_films   4/98343\n",
       "537                                 1951_films   1/98343\n",
       "\n",
       "[538 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ranking_Page(category,edges,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
